<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT From Scratch</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>GPT From Scratch (Inspired by Andrej Karpathy)</h1>
        </header>
        <main>
            <p>This project is an attempt to build a GPT (Generative Pre-trained Transformer) model from the ground up, drawing inspiration from Andrej Karpathy's insightful tutorial and code. The goal is to gain a deeper understanding of the inner workings of these powerful language models by implementing them using basic libraries.</p>

            <section>
                <h2>What is GPT?</h2>
                <p>GPT models are a type of large language model (LLM) that use deep learning, specifically a transformer architecture, to generate human-quality text. They can be used for tasks like:</p>
                <ul>
                    <li>Text generation: Writing stories, articles, or even code.</li>
                    <li>Translation: Converting text from one language to another.</li>
                    <li>Question answering: Providing answers to questions based on given information.</li>
                    <li>Summarization: Condensing large amounts of text into shorter summaries.</li>
                </ul>
            </section>

            <section>
                <h2>What Was Implemented</h2>
                <p>In this project, the core components of a GPT model were implemented, including:</p>
                <ul>
                    <li><strong>Transformer encoder blocks:</strong> The fundamental building blocks of the GPT architecture, responsible for processing and encoding the input text.</li>
                    <li><strong>Self-attention mechanism:</strong> This allows the model to weigh the importance of different words in the input when generating output, enabling it to capture context and relationships between words.</li>
                    <li><strong>Feedforward neural networks:</strong> These networks process the encoded information from the self-attention mechanism to generate the final output.</li>
                </ul>
            </section>

            <section>
                <h2>How to Run This Project</h2>
                <p>To run this project, you'll need to have Python 3.7 or higher installed, along with the dependencies listed in the `requirements.txt` file.</p>
                <ol>
                    <li><strong>Clone the repository:</strong>
                       <pre><code>git clone https://github.com/your-username/your-repo-name.git</code></pre>
                    </li>
                    <li><strong>Install dependencies:</strong>
                       <pre><code>pip install -r requirements.txt</code></pre>
                    </li>
                    <li><strong>Run the training script:</strong>
                       <pre><code>python train.py</code></pre>
                       <p>This script will train the GPT model on a small text dataset. You can modify the script to use a different dataset or adjust the model's hyperparameters.</p>
                    </li>
                </ol>
            </section>

            <section>
                <h2>Generated Text Example</h2>
                <p>Here is an example of text generated by the model:</p>
                <img src="https://github.com/user-attachments/assets/af79ea3d-8836-483e-92ea-e3bcba40a3a5" alt="generated text">
            </section>

            <section>
                <h2>Additional Notes</h2>
                <ul>
                    <li>This project is intended for educational purposes and to provide a deeper understanding of GPT models. It is not meant to be a production-ready implementation.</li>
                    <li>The model is trained on a small dataset due to resource constraints. Training on larger datasets would require significantly more compute power and time.</li>
                    <li>Comments are included throughout the code to explain the logic and implementation details.</li>
                </ul>
            </section>

            <section>
                <h2>Acknowledgments</h2>
                <ul>
                    <li>Andrej Karpathy for his fantastic tutorial and code, which served as the foundation for this project.</li>
                    <li>The creators and maintainers of the open-source libraries used in this project, including NumPy and PyTorch.</li>
                </ul>
            </section>
        </main>
        <footer>
            <p>This project is licensed under the MIT License.</p>
        </footer>
    </div>
</body>
</html>
